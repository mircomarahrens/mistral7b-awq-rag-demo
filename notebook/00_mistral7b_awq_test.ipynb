{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mircomarahrens/.cache/pypoetry/virtualenvs/mistral7b-awq-rag-demo-wl7TFlgK-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.83it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:07<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> Tell me about AI\n",
      "\n",
      "# The Future of AI: Five Ways to Prepare Your Business\n",
      "\n",
      "We are on the cusp of a new era of AI-driven innovation. As we look forward, it’s time to ask ourselves what the future of AI looks like. How can we prepare ourselves and our businesses for the new and exciting AI opportunities that lie ahead?\n",
      "\n",
      "In this post, we’ll explore five ways that you can prepare your business for the future of AI. By implementing these strategies, you’ll be well-positioned to take advantage of the opportunities that AI will bring.\n",
      "\n",
      "## 1. Embrace the potential of AI\n",
      "\n",
      "The first step in preparing for the future of AI is to embrace its potential. AI has the potential to transform businesses in a variety of ways. It can improve efficiency, increase productivity, and create new products and services.\n",
      "\n",
      "To take advantage of these opportunities, businesses must be willing to embrace AI and its potential. This means being open to new ideas and being willing to experiment. It also means being willing to invest in AI-related technologies and training.\n",
      "\n",
      "## 2. Educate your employees about AI\n",
      "\n",
      "Another way to prepare your business for the future of AI is to educate your employees about it. AI is a complex and rapidly evolving field. To make the most of AI, businesses must ensure that their employees have a basic understanding of it.\n",
      "\n",
      "There are a number of ways to educate your employees about AI. You can offer training courses, provide resources, and hold seminars. You can also encourage employees to learn about AI on their own time.\n",
      "\n",
      "## 3. Incorporate AI into your business strategy\n",
      "\n",
      "In addition to educating your employees about AI, you should also incorporate it into your business strategy. AI can be used to improve a variety of business processes, from customer service to supply chain management.\n",
      "\n",
      "To take advantage of AI, you must be willing to integrate it into your business strategy. This means being open to new ways of doing things and being willing to experiment. It also means being willing to invest in AI-related technologies and training.\n",
      "\n",
      "## 4. Prepare for the ethical implications of AI\n",
      "\n",
      "As AI becomes more prevalent, businesses must also be prepared for the ethical implications of AI. AI has the potential to cause harm, and businesses must be aware of the potential risks.\n",
      "\n",
      "To prepare for the ethical implications of AI, businesses must be willing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Inference should be possible with transformers pipeline as well in future\\n# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\\nfrom transformers import pipeline\\n\\nprint(\"*** Pipeline:\")\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    max_new_tokens=512,\\n    do_sample=True,\\n    temperature=0.7,\\n    top_p=0.95,\\n    top_k=40,\\n    repetition_penalty=1.1\\n)\\n\\nprint(pipe(prompt_template)[0][\\'generated_text\\'])\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"../model/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))\n",
    "\n",
    "\"\"\"\n",
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
