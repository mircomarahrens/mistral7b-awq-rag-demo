{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retrieval-augmented generation with open-source large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Retrieval-augmented generation (RAG) is a framework that uses pre-trained large language models (LLMs) to generate responses to queries grounded by an external knowledge base. The technique is described in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "We will use this framework to build a information retrieval system (IRS) realized by a chat prompt that will be able to answer questions supported by documents. The following sequence diagram depicts the general querying flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "    App->>Orchestrator: Prompt\n",
    "    Orchestrator->>Search: Query prompt\n",
    "    Search->>Corpus: Similiarity search\n",
    "    Corpus->>Search: Return content\n",
    "    Search->>Orchestrator: Retrieve content\n",
    "    Orchestrator->>GPT: Send prompt and content\n",
    "    GPT->>GPT: Reasoning and response generation\n",
    "    GPT->>Orchestrator: Return response\n",
    "    Orchestrator->>App: Return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The orchestrator is the integration code. We will use [LangChain](https://www.langchain.com/) here to coordinate the workflow. Our search service can be either a managed or some kind of self-hosted service. For now, we will go for a self-hosted service. The corpus service is our knowledge base. It is a vector database enabled to build an index for the similarity search from given data. The index is built by chunking our provided data and constructing embeddings from the chunks. This will be realized by using [Faiss](https://faiss.ai/index.html), [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/) and [Transformers](https://huggingface.co/docs/transformers/index). Our corpus is made of documents. Each document will have a unique id and contains some sort of text. Said that, a document can be any kind of text. It can be single words, an newspaper article or the whole encyclopedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema.runnable.passthrough import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited memory of our available GPU we need to quantize our model. Quantization is a technique to reduce the memory footprint of a model tremendously. This is achieved by reducing the precision of the model's weights. It enables us to use a larger model. Further details about the technique are described in the papers [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) and [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# !git lfs clone https://huggingface.co/mistralai/Mistral-7B-v0.1 ../resource/model/Mistral-7B-v0.1\n",
    "# WARNING: 'git lfs clone' is deprecated and will not be updated\n",
    "#           with new flags from 'git clone'\n",
    "\n",
    "# 'git clone' has been updated in upstream Git to have comparable\n",
    "# speeds to 'git lfs clone'.\n",
    "# Cloning into '../resource/model/Mistral-7B-v0.1'...\n",
    "# remote: Enumerating objects: 79, done.\n",
    "# remote: Counting objects: 100% (75/75), done.\n",
    "# remote: Compressing objects: 100% (74/74), done.\n",
    "# remote: Total 79 (delta 39), reused 0 (delta 0), pack-reused 4\n",
    "# Unpacking objects: 100% (79/79), 470.69 KiB | 2.39 MiB/s, done.\n",
    "# Downloading LFS objects: 100% (3/3), 15 GB | 32 MB/s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../resource/model/Mistral-7B-v0.1\u001b[0m\n",
      "├── README.md\n",
      "├── config.json\n",
      "├── generation_config.json\n",
      "├── pytorch_model-00001-of-00002.bin\n",
      "├── pytorch_model-00002-of-00002.bin\n",
      "├── pytorch_model.bin.index.json\n",
      "├── special_tokens_map.json\n",
      "├── tokenizer.json\n",
      "├── tokenizer.model\n",
      "└── tokenizer_config.json\n",
      "\n",
      "0 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 1 ../resource/model/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28G\t../resource/model/Mistral-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../resource/model/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, driver_version, memory.total [MiB]\n",
      "NVIDIA GeForce RTX 3060, 546.29, 12288 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our downloaded, unquantized model has a size of ~28 GB, whereas our GPU has only a memory of ~12 GB. Therefore, we will use quantization to reduce the model's size to ~7 GB. This will enable us to use the model on the given GPU. The technique is described in the blog post [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To achieve this we have to set some preconfigurations for our model loading with the help of the library [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Retrieving the availabile compute dtype from torch\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the bitsandbytes config\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd48eb3ed5542b8865995c4be8b1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading our model\n",
    "model_name_or_path = \"../resource/model/Mistral-7B-v0.1\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name 'model_name_or_path' already indicates, that we also could use a model directly from the [Hugging Face model hub](https://huggingface.co/models) and download it on demand. This is handy in situation where if we would like to check different models and it ensures that we always use the latest version of the model. Anyhow, for this tutorial we will use the model code we already have downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code checks how much memory has been occupied by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 6447 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will setup a pipeline for our communication with the model on the GPU. Therefore we need to initializing a [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). The tokenizer is used to convert our text input into a numerical representation that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # TODO explain this\n",
    "tokenizer.padding_side = \"right\" # TODO explain this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here shall be to perform the generation of text (\"text-generation\"). Other forms of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The other configuration parameters are:\n",
    "\n",
    "- temperature: Determines the randomness of the generated text. The higher the value, the more random the text. The lower the value, the more conservative the text.\n",
    "- repetition_penalty: Determines how much the model will avoid repeating the same word. The higher the value, the more likely the model will avoid repeating the same word.\n",
    "- return_full_text: Determines if the output should be the full text or only the generated part.\n",
    "- max_new_tokens: Determines the maximum number of tokens that can be generated. This is a safety measure to avoid infinite loops.\n",
    "- do_sample: Determines if the model should use sampling or greedy decoding. Sampling is more creative, but greedy decoding is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup index with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "document = \"../resource/data/example-docs/lufthansa-abb-07-2021-en.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(document)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sentence_transformer = \"../resource/model/all-mpnet-base-v2\"\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "database = FAISS.from_documents(\n",
    "    chunked_documents, HuggingFaceEmbeddings(model_name=sentence_transformer)\n",
    ")\n",
    "\n",
    "# Prepare the db to serve as retriever\n",
    "retriever = database.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I get a refund for a lost ticket?\"\n",
    "prompt_template = \"\"\"Answer the question. Use only the following corpus for your answer: {corpus}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"corpus\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "rag_chain = ( \n",
    " {\"corpus\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(\"Can I get a refund for a lost ticket?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Can I get a refund for a lost ticket?'\n",
      "('Chosen answer:\\n'\n",
      " 'Yes, you can apply for a refund of the lost ticket, provided that:\\n'\n",
      " '1) the ticket or portion thereof has not been used\\n'\n",
      " '2) the ticket has not been lost for more than 5 years\\n'\n",
      " '3) you present lost ticket as evidence of loss, and\\n'\n",
      " '4) you undertake in writing to repay to us the amount refunded in the event '\n",
      " 'that the lost ticket or portion thereof is presented and redeemed by some '\n",
      " 'other person.\\n'\n",
      " '\\n'\n",
      " '## Answer - Add one\\n'\n",
      " '| Question | Chosen answer |')\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "pp.pprint(answer['question'])\n",
    "pp.pprint(answer['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
